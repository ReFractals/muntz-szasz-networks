{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Müntz-Szász Networks (MSN) - Demo Notebook\n",
    "\n",
    "This notebook demonstrates the core capabilities of **Müntz-Szász Networks**, a neural network architecture with learnable fractional power bases.\n",
    "\n",
    "**Paper**: \"Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases\"  \n",
    "**Author**: Gnankan Landry Regis N'guessan\n",
    "\n",
    "## Contents\n",
    "1. Installation & Setup\n",
    "2. Basic MSN Usage\n",
    "3. Supervised Learning: Approximating √x\n",
    "4. Interpretability: Examining Learned Exponents\n",
    "5. PINN Example: Singular ODE\n",
    "6. Comparison with MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MSN (if not already installed)\n",
    "# !pip install -e ..\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import MSN components\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from msn import MSN, MSNTrainer\n",
    "from msn.baselines import MLP, build_param_matched_mlp\n",
    "from msn.utils import count_params, dump_exponents\n",
    "\n",
    "# Setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Plotting style\n",
    "plt.rcParams['figure.figsize'] = (10, 4)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic MSN Usage\n",
    "\n",
    "MSN replaces fixed activations with learnable power functions:\n",
    "\n",
    "$$\\phi(x) = \\sum_k a_k |x|^{\\mu_k} + \\sum_k b_k \\text{sgn}(x)|x|^{\\lambda_k}$$\n",
    "\n",
    "where $\\mu_k, \\lambda_k$ are **learned exponents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple MSN\n",
    "model = MSN(\n",
    "    dims=[1, 8, 8, 1],  # Architecture: 1 -> 8 -> 8 -> 1\n",
    "    Ke=6,               # 6 even exponents per edge\n",
    "    Ko=6,               # 6 odd exponents per edge\n",
    "    p_max_even=4.0,     # Maximum exponent value\n",
    "    exponent_mode=\"bounded\"  # Stable parameterization\n",
    ")\n",
    "\n",
    "print(f\"MSN Parameters: {count_params(model):,}\")\n",
    "print(f\"\\nArchitecture: {model.dims}\")\n",
    "print(f\"Number of layers: {len(model.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine initial (random) exponents\n",
    "exp_dict = dump_exponents(model, layer_idx=0)\n",
    "print(\"Initial even exponents (μ):\", [f\"{x:.3f}\" for x in exp_dict['mu']])\n",
    "print(\"Initial odd exponents (λ):\", [f\"{x:.3f}\" for x in exp_dict['lam']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Supervised Learning: Approximating √x\n",
    "\n",
    "The function $f(x) = \\sqrt{x}$ has a singular derivative at $x=0$. This is challenging for standard MLPs but natural for MSN (which can learn $\\mu \\approx 0.5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "def f_sqrt(x):\n",
    "    return torch.sqrt(x + 1e-12)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "n_train = 2048\n",
    "x_train = torch.rand(n_train, 1).to(device)\n",
    "y_train = f_sqrt(x_train)\n",
    "\n",
    "# Test data (uniform grid)\n",
    "x_test = torch.linspace(0, 1, 500).view(-1, 1).to(device)\n",
    "y_test = f_sqrt(x_test)\n",
    "\n",
    "print(f\"Training samples: {n_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train MSN\n",
    "msn_model = MSN(\n",
    "    dims=[1, 8, 8, 1],\n",
    "    Ke=6, Ko=6,\n",
    "    p_max_even=2.0,  # Restrict to small exponents for sqrt\n",
    "    exponent_mode=\"bounded\"\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(msn_model.parameters(), lr=2e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for step in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    pred = msn_model(x_train)\n",
    "    loss = criterion(pred, y_train)\n",
    "    \n",
    "    # Add Müntz regularizer\n",
    "    loss = loss + 0.01 * msn_model.muntz_regularizer(C=2.0)\n",
    "    loss = loss + 1e-4 * msn_model.l1_coeff_regularizer()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    if step % 500 == 0:\n",
    "        print(f\"Step {step}: loss = {loss.item():.6f}\")\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and visualize\n",
    "msn_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = msn_model(x_test)\n",
    "    rmse = torch.sqrt(torch.mean((y_pred - y_test)**2)).item()\n",
    "\n",
    "print(f\"MSN Test RMSE: {rmse:.5f}\")\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Solution\n",
    "axes[0].plot(x_test.cpu(), y_test.cpu(), 'b-', label=r'True: $\\sqrt{x}$', linewidth=2)\n",
    "axes[0].plot(x_test.cpu(), y_pred.cpu(), 'r--', label='MSN prediction', linewidth=2)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title(r'MSN Approximation of $f(x) = \\sqrt{x}$')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error\n",
    "error = torch.abs(y_pred - y_test).cpu()\n",
    "axes[1].semilogy(x_test.cpu(), error + 1e-10, 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('|error|')\n",
    "axes[1].set_title('Pointwise Error (log scale)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interpretability: Examining Learned Exponents\n",
    "\n",
    "A key advantage of MSN is that learned exponents reveal the solution structure. For $\\sqrt{x}$, we expect $\\mu \\approx 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get learned exponents\n",
    "exp_dict = dump_exponents(msn_model, layer_idx=0)\n",
    "\n",
    "print(\"Learned even exponents (μ):\")\n",
    "for i, mu in enumerate(exp_dict['mu']):\n",
    "    marker = \"  ← CLOSE TO 0.5!\" if abs(mu - 0.5) < 0.1 else \"\"\n",
    "    print(f\"  μ_{i+1} = {mu:.4f}{marker}\")\n",
    "\n",
    "print(f\"\\nTarget exponent for √x: α = 0.5\")\n",
    "print(f\"Closest learned exponent: μ = {min(exp_dict['mu'], key=lambda x: abs(x-0.5)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize exponent distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "mu_vals = exp_dict['mu']\n",
    "lam_vals = exp_dict['lam']\n",
    "\n",
    "ax.scatter(mu_vals, [1]*len(mu_vals), s=200, c='green', marker='o', label='Even (μ)', alpha=0.7)\n",
    "ax.scatter(lam_vals, [0.5]*len(lam_vals), s=200, c='orange', marker='s', label='Odd (λ)', alpha=0.7)\n",
    "\n",
    "# Mark target\n",
    "ax.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label=r'Target $\\alpha=0.5$')\n",
    "\n",
    "ax.set_xlabel('Exponent value')\n",
    "ax.set_yticks([0.5, 1])\n",
    "ax.set_yticklabels(['Odd (λ)', 'Even (μ)'])\n",
    "ax.set_title(r'Learned Exponents for $f(x) = \\sqrt{x}$')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.set_xlim(0, 2.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PINN Example: Singular ODE\n",
    "\n",
    "Physics-Informed Neural Networks (PINNs) solve differential equations by embedding physics into the loss. Consider:\n",
    "\n",
    "$$u'(x) = \\frac{1}{2\\sqrt{x}}, \\quad u(0) = 0$$\n",
    "\n",
    "The solution is $u(x) = \\sqrt{x}$, which has a **singular derivative at $x=0$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_loss(model, n_col=2048):\n",
    "    \"\"\"PINN loss for u'(x) = 1/(2√x), u(0) = 0\"\"\"\n",
    "    # Collocation points (biased toward x=0)\n",
    "    x = torch.rand(n_col, 1, device=device) ** 2\n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "    u = model(x)\n",
    "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
    "    \n",
    "    # PDE residual: u' - 1/(2√x) = 0\n",
    "    rhs = 0.5 / torch.sqrt(x + 1e-12)\n",
    "    pde_loss = torch.mean((u_x - rhs) ** 2)\n",
    "    \n",
    "    # Boundary condition: u(0) = 0\n",
    "    x0 = torch.zeros(256, 1, device=device)\n",
    "    bc_loss = torch.mean(model(x0) ** 2)\n",
    "    \n",
    "    return pde_loss, bc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MSN for PINN\n",
    "pinn_msn = MSN(\n",
    "    dims=[1, 8, 8, 1],\n",
    "    Ke=6, Ko=6,\n",
    "    p_max_even=3.0,\n",
    "    exponent_mode=\"bounded\"\n",
    ").to(device)\n",
    "\n",
    "# Use MSNTrainer for stable training\n",
    "trainer = MSNTrainer(\n",
    "    pinn_msn,\n",
    "    lr=1e-3,\n",
    "    lr_exp_mult=0.02,\n",
    "    warmup_steps=500,\n",
    "    exp_grad_clip=0.05,\n",
    "    use_muntz=True,\n",
    "    use_l1=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "pinn_losses = []\n",
    "for step in range(3000):\n",
    "    pde_loss, bc_loss = pinn_loss(pinn_msn)\n",
    "    total_loss = pde_loss + 200 * bc_loss\n",
    "    \n",
    "    metrics = trainer.step(total_loss)\n",
    "    pinn_losses.append(metrics['loss'])\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        warmup_status = \"(warmup)\" if not metrics['warmup_done'] else \"\"\n",
    "        print(f\"Step {step}: loss={metrics['loss']:.6f} {warmup_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PINN solution\n",
    "pinn_msn.eval()\n",
    "with torch.no_grad():\n",
    "    pinn_pred = pinn_msn(x_test)\n",
    "    pinn_rmse = torch.sqrt(torch.mean((pinn_pred - y_test)**2)).item()\n",
    "\n",
    "print(f\"PINN MSN Test RMSE: {pinn_rmse:.5f}\")\n",
    "\n",
    "# Check learned exponents\n",
    "pinn_exp = dump_exponents(pinn_msn, layer_idx=0)\n",
    "closest_mu = min(pinn_exp['mu'], key=lambda x: abs(x-0.5))\n",
    "print(f\"Closest exponent to 0.5: μ = {closest_mu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison with MLP\n",
    "\n",
    "Let's compare MSN with a standard MLP (parameter-matched)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter-matched MLP\n",
    "msn_params = count_params(msn_model)\n",
    "mlp_model, mlp_params, H = build_param_matched_mlp(\n",
    "    Din=1, Dout=1, target_params=msn_params, depth=3\n",
    ")\n",
    "mlp_model = mlp_model.to(device)\n",
    "\n",
    "print(f\"MSN params: {msn_params}\")\n",
    "print(f\"MLP params: {mlp_params} (H={H})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP\n",
    "mlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=2e-3)\n",
    "\n",
    "mlp_losses = []\n",
    "for step in range(2000):\n",
    "    mlp_optimizer.zero_grad()\n",
    "    pred = mlp_model(x_train)\n",
    "    loss = criterion(pred, y_train)\n",
    "    loss.backward()\n",
    "    mlp_optimizer.step()\n",
    "    mlp_losses.append(loss.item())\n",
    "\n",
    "# Evaluate MLP\n",
    "mlp_model.eval()\n",
    "with torch.no_grad():\n",
    "    mlp_pred = mlp_model(x_test)\n",
    "    mlp_rmse = torch.sqrt(torch.mean((mlp_pred - y_test)**2)).item()\n",
    "\n",
    "print(f\"MLP Test RMSE: {mlp_rmse:.5f}\")\n",
    "print(f\"MSN Test RMSE: {rmse:.5f}\")\n",
    "print(f\"\\nMSN improvement: {mlp_rmse/rmse:.1f}×\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Solutions\n",
    "axes[0].plot(x_test.cpu(), y_test.cpu(), 'b-', label=r'True $\\sqrt{x}$', linewidth=2)\n",
    "axes[0].plot(x_test.cpu(), y_pred.cpu(), 'g--', label=f'MSN (RMSE={rmse:.4f})', linewidth=2)\n",
    "axes[0].plot(x_test.cpu(), mlp_pred.cpu(), 'r:', label=f'MLP (RMSE={mlp_rmse:.4f})', linewidth=2)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Solution Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error comparison\n",
    "msn_error = torch.abs(y_pred - y_test).cpu()\n",
    "mlp_error = torch.abs(mlp_pred - y_test).cpu()\n",
    "\n",
    "axes[1].semilogy(x_test.cpu(), msn_error + 1e-10, 'g-', label='MSN error', linewidth=2)\n",
    "axes[1].semilogy(x_test.cpu(), mlp_error + 1e-10, 'r--', label='MLP error', linewidth=2)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('|error|')\n",
    "axes[1].set_title('Pointwise Error (log scale)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning curves\n",
    "axes[2].semilogy(losses, 'g-', label='MSN', alpha=0.8)\n",
    "axes[2].semilogy(mlp_losses, 'r-', label='MLP', alpha=0.8)\n",
    "axes[2].set_xlabel('Step')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].set_title('Training Loss')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This demo showed:\n",
    "\n",
    "1. **MSN learns interpretable exponents**: For $\\sqrt{x}$, MSN learns $\\mu \\approx 0.5$\n",
    "2. **MSN outperforms MLP on singular functions**: Achieving lower error with the same parameters\n",
    "3. **Stable training techniques**: Warmup, two-time-scale optimization, gradient clipping\n",
    "4. **PINN integration**: MSN works naturally with physics-informed learning\n",
    "\n",
    "For more details, see the paper and full experiments in `experiments/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
